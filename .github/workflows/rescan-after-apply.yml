name: Security Pipeline - 04 Verify FAIL Reduction

on:
  workflow_dispatch:
    inputs:
      triggered_by:
        description: "Trigger reason (e.g. manual, no-remediation-needed)"
        required: false
        default: "manual"
  workflow_run:
    workflows: ["Security Pipeline - 03 Apply Merged Generated Terraform Remediation"]
    types: [completed]

concurrency:
  group: security-pipeline-04-rescan-${{ github.ref }}
  cancel-in-progress: true

permissions:
  id-token: write
  contents: read
  pull-requests: write
  actions: read

jobs:
  rescan:
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: ap-northeast-2

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "prowler>=3.11,<4.0"

      - name: Wait for eventual consistency
        run: sleep 150

      - name: Run post-apply Prowler (Seoul + CIS/ISMS-P)
        run: |
          mkdir -p artifacts/rescan
          prowler aws --region ap-northeast-2 --compliance cis_1.4_aws -M json-asff -o artifacts/rescan -F post_cis || true
          CHECKS="$(tr '\n' ' ' < iac/compliance/isms_p_checks.txt | xargs)"
          if [ -n "$CHECKS" ]; then
            prowler aws --region ap-northeast-2 -c $CHECKS -M json-asff -o artifacts/rescan -F post_isms_p || true
          fi
          python iac/scripts/merge_prowler_outputs.py \
            --dir artifacts/rescan \
            --prefix post \
            --output artifacts/rescan/post.asff.json
          python iac/scripts/normalize_findings.py \
            --input artifacts/rescan/post.asff.json \
            --output artifacts/rescan/post_normalized.json \
            --region ap-northeast-2

      - name: Download latest successful baseline scan artifact
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          mkdir -p artifacts/baseline_scan
          RID="$(gh run list --workflow "Security Pipeline - 01 Scan Baseline" --repo "${{ github.repository }}" --limit 30 --json databaseId,conclusion --jq '[.[] | select(.conclusion=="success")][0].databaseId')"
          if [ -n "$RID" ] && [ "$RID" != "null" ]; then
            gh run download "$RID" --repo "${{ github.repository }}" --name "scan-$RID" --dir artifacts/baseline_scan || true
          fi

      - name: Download previous successful rescan artifact (for incremental delta)
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          mkdir -p artifacts/previous_rescan
          PREV_RID="$(gh run list --workflow "Security Pipeline - 04 Verify FAIL Reduction" --repo "${{ github.repository }}" --limit 30 --json databaseId,conclusion --jq '[.[] | select(.conclusion=="success" and (.databaseId|tostring)!="${{ github.run_id }}")][0].databaseId')"
          if [ -n "$PREV_RID" ] && [ "$PREV_RID" != "null" ]; then
            echo "previous_rescan_run_id=$PREV_RID" >> "$GITHUB_ENV"
            gh run download "$PREV_RID" --repo "${{ github.repository }}" --name "rescan-$PREV_RID" --dir artifacts/previous_rescan || true
          fi

      - name: Compare baseline vs post
        run: |
          BASELINE_DOC="remediation/manifest.json"
          if [ -f "artifacts/baseline_scan/scan_manifest.json" ]; then
            BASELINE_DOC="artifacts/baseline_scan/scan_manifest.json"
          fi
          python iac/scripts/compare_failures.py \
            --baseline "$BASELINE_DOC" \
            --post artifacts/rescan/post_normalized.json \
            --output artifacts/rescan/rescan_summary.json
          python - <<'PY'
          import json
          from pathlib import Path
          summary_path = Path('artifacts/rescan/rescan_summary.json')
          s = json.loads(summary_path.read_text(encoding='utf-8'))
          current_post = json.loads(Path('artifacts/rescan/post_normalized.json').read_text(encoding='utf-8'))
          current_fail_rows = [x for x in current_post if x.get('status') == 'FAIL']
          current_fail_ids = sorted({x.get('check_id', 'unknown') for x in current_fail_rows})
          manual_override_ids = set()
          merged_manifest = Path('remediation/manifest.json')
          if merged_manifest.exists():
            try:
              merged_doc = json.loads(merged_manifest.read_text(encoding='utf-8'))
              for cat in merged_doc.get('categories', []) or []:
                for cid in cat.get('manual_required', []) or []:
                  cid_s = str(cid).strip()
                  if cid_s:
                    manual_override_ids.add(cid_s)

              check_map = merged_doc.get('check_map', {}) or {}
              for cid, entry in check_map.items():
                entries = entry if isinstance(entry, list) else [entry]
                for e in entries:
                  tier = str((e or {}).get('remediation_tier', '')).strip().lower()
                  if tier == 'manual-runbook':
                    cid_s = str(cid).strip()
                    if cid_s:
                      manual_override_ids.add(cid_s)
            except Exception:
              pass
          for cat_manifest in Path("remediation").glob("*/manifest.json"):
            try:
              cat_doc = json.loads(cat_manifest.read_text(encoding='utf-8'))
              for item in cat_doc.get("items", []) or []:
                cid = str(item.get("check_id", "")).strip()
                if cid and bool(item.get("manual_required")):
                  manual_override_ids.add(cid)
            except Exception:
              pass

          def is_manual_runbook(row):
            tier = str(row.get('remediation_tier', '')).strip().lower()
            cid = str(row.get('check_id', '')).strip()
            return (
              bool(row.get('manual_required'))
              or bool(row.get('non_terraform'))
              or tier == 'manual-runbook'
              or cid in manual_override_ids
            )

          total_fail_counts = {}
          remediable_fail_counts = {}
          manual_fail_counts = {}
          for row in current_fail_rows:
            cid = str(row.get('check_id', 'unknown'))
            total_fail_counts[cid] = total_fail_counts.get(cid, 0) + 1
            if is_manual_runbook(row):
              manual_fail_counts[cid] = manual_fail_counts.get(cid, 0) + 1
            else:
              remediable_fail_counts[cid] = remediable_fail_counts.get(cid, 0) + 1

          current_remediable_fail = len([r for r in current_fail_rows if not is_manual_runbook(r)])
          current_manual_fail = len([r for r in current_fail_rows if is_manual_runbook(r)])
          s['post_fail_remediable'] = current_remediable_fail
          s['post_fail_manual_runbook'] = current_manual_fail

          prev_summary_path = Path('artifacts/previous_rescan/rescan_summary.json')
          prev_post_path = Path('artifacts/previous_rescan/post_normalized.json')
          previous_post_fail = None
          incremental_reduced = None
          previous_post_remediable_fail = None
          incremental_reduced_remediable = None
          resolved_since_previous = []
          new_fail_since_previous = []
          resolved_since_previous_remediable = []
          new_fail_since_previous_remediable = []

          if prev_summary_path.exists():
            prev_summary = json.loads(prev_summary_path.read_text(encoding='utf-8'))
            previous_post_fail = int(prev_summary.get('post_fail', 0))
            incremental_reduced = previous_post_fail - int(s.get('post_fail', 0))
            s['previous_post_fail'] = previous_post_fail
            s['incremental_reduced'] = incremental_reduced
            if 'post_fail_remediable' in prev_summary:
              previous_post_remediable_fail = int(prev_summary.get('post_fail_remediable', 0))
              incremental_reduced_remediable = previous_post_remediable_fail - int(current_remediable_fail)
              s['previous_post_fail_remediable'] = previous_post_remediable_fail
              s['incremental_reduced_remediable'] = incremental_reduced_remediable

          if prev_post_path.exists():
            prev_post = json.loads(prev_post_path.read_text(encoding='utf-8'))
            prev_fail_rows = [x for x in prev_post if x.get('status') == 'FAIL']
            prev_fail_ids = sorted({x.get('check_id', 'unknown') for x in prev_fail_rows})
            prev_remediable_ids = sorted({x.get('check_id', 'unknown') for x in prev_fail_rows if not is_manual_runbook(x)})
            current_remediable_ids = sorted({x.get('check_id', 'unknown') for x in current_fail_rows if not is_manual_runbook(x)})
            resolved_since_previous = sorted(set(prev_fail_ids) - set(current_fail_ids))
            new_fail_since_previous = sorted(set(current_fail_ids) - set(prev_fail_ids))
            s['resolved_since_previous_check_ids'] = resolved_since_previous
            s['new_fail_since_previous_check_ids'] = new_fail_since_previous
            resolved_since_previous_remediable = sorted(set(prev_remediable_ids) - set(current_remediable_ids))
            new_fail_since_previous_remediable = sorted(set(current_remediable_ids) - set(prev_remediable_ids))
            s['resolved_since_previous_remediable_check_ids'] = resolved_since_previous_remediable
            s['new_fail_since_previous_remediable_check_ids'] = new_fail_since_previous_remediable
            if previous_post_remediable_fail is None:
              previous_post_remediable_fail = len(prev_remediable_ids)
              incremental_reduced_remediable = previous_post_remediable_fail - int(current_remediable_fail)
              s['previous_post_fail_remediable'] = previous_post_remediable_fail
              s['incremental_reduced_remediable'] = incremental_reduced_remediable

          MANUAL_REMEDIATION_GUIDE = {
            'prowler-s3_bucket_no_mfa_delete': {
              'reason': 'S3 MFA Delete í™œì„±í™”ëŠ” ë£¨íŠ¸ ê³„ì • ìžê²©ì¦ëª…+MFA í† í°ì´ ë™ì‹œì— í•„ìš”í•˜ë©°, Terraform/SDKì—ì„œ ë£¨íŠ¸ ì„¸ì…˜ì„ ì§€ì›í•˜ì§€ ì•Šì•„ ìžë™í™” ë¶ˆê°€',
              'guide': '1. ë£¨íŠ¸ ê³„ì •ìœ¼ë¡œ AWS CLI ë¡œê·¸ì¸ â†’ 2. aws s3api put-bucket-versioning --bucket {ë²„í‚·ëª…} --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa "arn:aws:iam::{ê³„ì •ID}:mfa/root-account-mfa {TOTPì½”ë“œ}"',
            },
            'prowler-iam_root_hardware_mfa_enabled': {
              'reason': 'í•˜ë“œì›¨ì–´ MFA ë””ë°”ì´ìŠ¤ ë“±ë¡ì€ ë¬¼ë¦¬ì  ë””ë°”ì´ìŠ¤ê°€ í•„ìš”í•˜ë©°, AWS ì½˜ì†”ì—ì„œë§Œ ì§ì ‘ ì¡°ìž‘ ê°€ëŠ¥',
              'guide': '1. AWS ì½˜ì†” â†’ IAM â†’ ë³´ì•ˆ ìžê²©ì¦ëª… â†’ 2. MFA ë””ë°”ì´ìŠ¤ â†’ í•˜ë“œì›¨ì–´ MFA ë””ë°”ì´ìŠ¤ í™œì„±í™” â†’ 3. í˜¸í™˜ ìž¥ì¹˜: YubiKey(FIDO2), Gemalto í† í°',
            },
            'prowler-ec2_ebs_volume_encryption': {
              'reason': 'ê¸°ì¡´ EBS ë³¼ë¥¨ ì•”í˜¸í™” ì „í™˜ì€ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ ë° ë³¼ë¥¨ êµì²´ê°€ í•„ìš”í•˜ì—¬ ì„œë¹„ìŠ¤ ì˜í–¥ ë°œìƒ',
              'guide': '1. ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€ â†’ 2. ë³¼ë¥¨ ìŠ¤ëƒ…ìƒ· ìƒì„± â†’ 3. ìŠ¤ëƒ…ìƒ· ì•”í˜¸í™” ë³µì‚¬(KMS í‚¤ ì§€ì •) â†’ 4. ì•”í˜¸í™” ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ìƒˆ ë³¼ë¥¨ ìƒì„± â†’ 5. ê¸°ì¡´ ë³¼ë¥¨ ë¶„ë¦¬ í›„ ìƒˆ ë³¼ë¥¨ ì—°ê²° â†’ 6. ì¸ìŠ¤í„´ìŠ¤ ì‹œìž‘ ë° ê²€ì¦',
            },
            'prowler-ec2_instance_profile_attached': {
              'reason': 'EC2 ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼ ì—°ê²°ì€ ì‹¤í–‰ ì¤‘ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ê¶Œí•œì— ì˜í–¥ì„ ì£¼ì–´ ì‚¬ì „ ê²€í†  ë° ìˆ˜ë™ ì ìš© í•„ìš”',
              'guide': '1. ì ì ˆí•œ ê¶Œí•œì˜ IAM ì—­í•  ìƒì„± â†’ 2. EC2 ì½˜ì†” â†’ ì¸ìŠ¤í„´ìŠ¤ â†’ ìž‘ì—… â†’ ë³´ì•ˆ â†’ IAM ì—­í•  ìˆ˜ì • â†’ 3. ìƒì„±í•œ IAM ì—­í•  ì—°ê²° ë° ì €ìž¥',
            },
          }

          def make_comparison_section(title, asis_counts, is_counts, before_lbl, after_lbl):
            asis_lines = [f'**{before_lbl}**', '| ì ê²€ í•­ëª© ID | FAIL ìˆ˜ |', '|---|---:|']
            if asis_counts:
              for cid in sorted(asis_counts): asis_lines.append(f'| `{cid}` | {asis_counts[cid]} |')
            else:
              asis_lines.append('| ì—†ìŒ | 0 |')
            is_lines = [f'**{after_lbl}**', '| ì ê²€ í•­ëª© ID | FAIL ìˆ˜ |', '|---|---:|']
            if is_counts:
              for cid in sorted(is_counts): is_lines.append(f'| `{cid}` | {is_counts[cid]} |')
            else:
              is_lines.append('| ì—†ìŒ | 0 |')
            return (
              f'### {title}\n'
              + '\n'.join(asis_lines) + '\n\n**â†’**\n\n' + '\n'.join(is_lines)
            )

          def make_service_comparison(asis_rows, is_rows, before_lbl, after_lbl):
            def svc_counts(rows):
              c = {}
              for r in rows:
                svc = str(r.get('service', 'unknown')).lower().strip() or 'unknown'
                c[svc] = c.get(svc, 0) + 1
              return c
            asis_c = svc_counts(asis_rows)
            is_c = svc_counts(is_rows)
            all_svcs = sorted(set(list(asis_c.keys()) + list(is_c.keys())))
            asis_lines = [f'**{before_lbl}**', '| ì„œë¹„ìŠ¤ | FAIL ìˆ˜ |', '|---|---:|']
            for svc in all_svcs:
              if svc in asis_c: asis_lines.append(f'| `{svc}` | {asis_c[svc]} |')
            if not asis_c: asis_lines.append('| ì—†ìŒ | 0 |')
            is_lines = [f'**{after_lbl}**', '| ì„œë¹„ìŠ¤ | FAIL ìˆ˜ |', '|---|---:|']
            for svc in all_svcs:
              if svc in is_c: is_lines.append(f'| `{svc}` | {is_c[svc]} |')
            if not is_c: is_lines.append('| ì—†ìŒ | 0 |')
            return (
              '### ì„œë¹„ìŠ¤ë³„ FAIL ë¹„êµí‘œ\n'
              + '\n'.join(asis_lines) + '\n\n**â†’**\n\n' + '\n'.join(is_lines)
            )

          before_lbl = 'ðŸ”´ ì¡°ì¹˜ ì „ (ì´ì „ ìž¬ìŠ¤ìº” ê¸°ì¤€)' if prev_post_path.exists() else 'ðŸ”´ ì¡°ì¹˜ ì „ (ë² ì´ìŠ¤ë¼ì¸ ê¸°ì¤€)'
          after_lbl = 'ðŸŸ¢ ì¡°ì¹˜ í›„ (í˜„ìž¬ ìž¬ìŠ¤ìº” ê²°ê³¼)'
          if prev_post_path.exists():
            prev_fail_all = [x for x in json.loads(prev_post_path.read_text(encoding='utf-8')) if x.get('status') == 'FAIL']
          else:
            bl_path = Path('artifacts/baseline_scan/prioritized_findings.json')
            prev_fail_all = [x for x in json.loads(bl_path.read_text(encoding='utf-8')) if x.get('status') == 'FAIL'] if bl_path.exists() else []

          asis_total_counts = {}
          asis_remediable_counts = {}
          for row in prev_fail_all:
            cid = str(row.get('check_id', 'unknown'))
            asis_total_counts[cid] = asis_total_counts.get(cid, 0) + 1
            if not is_manual_runbook(row):
              asis_remediable_counts[cid] = asis_remediable_counts.get(cid, 0) + 1

          rem_table_lines = [
            '| ë‚¨ì€ ì ê²€ í•­ëª© | ê±´ìˆ˜ |',
            '|---|---:|',
          ]
          if s['remaining_fail_check_ids']:
            for cid in sorted(s['remaining_fail_check_ids']):
              rem_table_lines.append(f'| `{cid}` | {int(total_fail_counts.get(cid, 0))} |')
          else:
            rem_table_lines.append('| ì—†ìŒ | 0 |')

          remediable_table_lines = [
            '| ì ê²€ í•­ëª© (Terraform ê°€ëŠ¥) | ê±´ìˆ˜ |',
            '|---|---:|',
          ]
          if remediable_fail_counts:
            for cid in sorted(remediable_fail_counts):
              remediable_table_lines.append(f'| `{cid}` | {int(remediable_fail_counts.get(cid, 0))} |')
          else:
            remediable_table_lines.append('| ì—†ìŒ | 0 |')

          manual_table_lines = [
            '| ì ê²€ í•­ëª© (ìˆ˜ë™ ì¡°ì¹˜ í•„ìš”) | ê±´ìˆ˜ |',
            '|---|---:|',
          ]
          if manual_fail_counts:
            for cid in sorted(manual_fail_counts):
              manual_table_lines.append(f'| `{cid}` | {int(manual_fail_counts.get(cid, 0))} |')
          else:
            manual_table_lines.append('| ì—†ìŒ | 0 |')
          manual_table = '\n'.join(manual_table_lines)

          manual_guide_lines = []
          if manual_fail_counts:
            manual_guide_lines.append('\n**ìˆ˜ë™ ì¡°ì¹˜ ê°€ì´ë“œ:**')
            for cid in sorted(manual_fail_counts):
              guide_info = MANUAL_REMEDIATION_GUIDE.get(cid)
              if guide_info:
                manual_guide_lines.append(f'\n**`{cid}`**')
                manual_guide_lines.append(f'- **ìˆ˜ë™ ë¶„ë¥˜ ì´ìœ :** {guide_info["reason"]}')
                manual_guide_lines.append(f'- **ì¡°ì¹˜ ë°©ë²•:** {guide_info["guide"]}')
              else:
                manual_guide_lines.append(f'\n**`{cid}`**')
                manual_guide_lines.append('- í•´ë‹¹ í•­ëª©ì€ ì½˜ì†” ë˜ëŠ” ìˆ˜ë™ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.')

          now_lines = [
            '### ë‹¤ìŒ ì§„í–‰ í•­ëª© (ìžë™ ì¡°ì¹˜ ê°€ëŠ¥)',
          ]
          if remediable_fail_counts:
            now_lines.append(f'- Terraformë¡œ ê³„ì† ì¤„ì¼ ìˆ˜ ìžˆëŠ” FAIL: {current_remediable_fail}')
            now_lines.append('- ì¡°ì¹˜ ë°©í–¥: ë³´ì™„ PR ìƒì„± â†’ ë³‘í•©/ì ìš© â†’ ìž¬ìŠ¤ìº” ìˆœì„œë¡œ ë°˜ë³µ')
          else:
            now_lines.append('- Terraform ìžë™í™”ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œ FAILì€ ëª¨ë‘ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.')
            now_lines.append('- ë‹¤ìŒì€ ì•„ëž˜ ìˆ˜ë™ ì¡°ì¹˜ í•­ëª© ì¤‘ì‹¬ìœ¼ë¡œ ì§„í–‰í•˜ë©´ ë©ë‹ˆë‹¤.')

          later_lines = [
            '### ìˆ˜ë™ í™•ì¸/ìš´ì˜ì´ í•„ìš”í•œ í•­ëª©',
            f'- ìˆ˜ë™ ì¡°ì¹˜ê°€ í•„ìš”í•œ FAIL: {current_manual_fail}',
            '- ê³„ì • ì •ì±…, ì¡°ì§ í†µì œ, ì½˜ì†” ìž‘ì—…ì´ í•„ìš”í•œ í•­ëª©ìž…ë‹ˆë‹¤.',
          ]

          md = (
            '## 04. ìž¬ìŠ¤ìº” ê²°ê³¼ ìš”ì•½\n'
            f'- ê¸°ì¤€ì„  FAIL: {s["baseline_fail"]}\n'
            f'- í˜„ìž¬ FAIL: {s["post_fail"]}\n'
            f'- ì¤„ì–´ë“  FAIL: {s["reduced"]} (ê¸°ì¤€ì„  ëŒ€ë¹„)\n'
            f'- ìžë™ ì¡°ì¹˜ ê°€ëŠ¥ FAIL(Terraform): {current_remediable_fail}\n'
            f'- ìˆ˜ë™ ì¡°ì¹˜ í•„ìš” FAIL: {current_manual_fail}\n\n'
            + make_service_comparison(prev_fail_all, current_fail_rows, before_lbl, after_lbl) + '\n\n'
            + make_comparison_section('ì „ì²´ FAIL ë¹„êµí‘œ', asis_total_counts, total_fail_counts, before_lbl, after_lbl) + '\n\n'
            + make_comparison_section('ìžë™ ì¡°ì¹˜ ê°€ëŠ¥ FAIL ë¹„êµí‘œ (Terraform)', asis_remediable_counts, remediable_fail_counts, before_lbl, after_lbl) + '\n\n'
            + '### ìˆ˜ë™ ì¡°ì¹˜ í•„ìš” í•­ëª© (ì½˜ì†”/ìˆ˜ë™ ìš´ì˜ í•„ìš”)\n'
            + manual_table + '\n'
            + ('\n'.join(manual_guide_lines) if manual_guide_lines else '')
            + '\n\n'
            + '\n'.join(now_lines)
            + '\n\n'
            + '\n'.join(later_lines)
            + '\n'
          )
          summary_path.write_text(json.dumps(s, indent=2), encoding='utf-8')
          Path('artifacts/rescan/rescan_summary.md').write_text(md, encoding='utf-8')
          print(md)
          PY
          cat artifacts/rescan/rescan_summary.md >> "$GITHUB_STEP_SUMMARY"
          python - <<'PY'
          import json
          from pathlib import Path
          s = json.loads(Path('artifacts/rescan/rescan_summary.json').read_text(encoding='utf-8'))
          incremental = s.get("incremental_reduced_remediable")
          if incremental is None:
              incremental = s.get("incremental_reduced")
          new_remediable = s.get("new_fail_since_previous_remediable_check_ids", []) or []
          if incremental is not None:
              if incremental < 0:
                  if new_remediable:
                      raise SystemExit(
                          "FAIL increased with new terraform-capable checks since previous rescan: "
                          f"new={','.join(new_remediable)} "
                          f"previous_post_remediable={s.get('previous_post_fail_remediable', s.get('previous_post_fail'))} "
                          f"current_post_remediable={s.get('post_fail_remediable', s.get('post_fail'))}"
                      )
                  Path("artifacts/rescan/no_reduction.flag").write_text("1", encoding="utf-8")
                  print(
                      "WARNING: remediable fail count increased but no new check IDs "
                      "(count fluctuation/noise treated as neutral): "
                      f"previous_post_remediable={s.get('previous_post_fail_remediable', s.get('previous_post_fail'))} "
                      f"current_post_remediable={s.get('post_fail_remediable', s.get('post_fail'))}"
                  )
              if incremental == 0:
                  Path("artifacts/rescan/no_reduction.flag").write_text("1", encoding="utf-8")
                  print(
                      "WARNING: no additional reduction vs previous post (terraform-capable scope): "
                      f"previous_post_remediable={s.get('previous_post_fail_remediable', s.get('previous_post_fail'))} "
                      f"current_post_remediable={s.get('post_fail_remediable', s.get('post_fail'))}"
                  )
          elif s.get("reduced", 0) <= 0:
              raise SystemExit(f"FAIL not reduced vs baseline: baseline={s.get('baseline_fail')} post={s.get('post_fail')}")
          PY

      - name: Comment on merged PR when no additional reduction
        if: always()
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          if [ ! -f artifacts/rescan/no_reduction.flag ]; then
            echo "no neutral warning; skip PR comment"
            exit 0
          fi
          TITLE='${{ github.event.workflow_run.display_title }}'
          if [[ "$TITLE" =~ \#([0-9]+) ]]; then
            PR_NUMBER="${BASH_REMATCH[1]}"
          else
            echo "could not parse PR number from title: $TITLE"
            exit 0
          fi
          MSG=$'No additional FAIL reduction from this merge.\n\n- Result: neutral (apply succeeded, but fail count unchanged)\n- See workflow summary/artifact `rescan-${{ github.run_id }}` for remaining checks.'
          gh pr comment "$PR_NUMBER" --repo "${{ github.repository }}" --body "$MSG" || true

      - name: Optional publish rescan result to external API
        continue-on-error: true
        env:
          PROWLER_APP_API_URL: ${{ secrets.PROWLER_APP_API_URL }}
          PROWLER_APP_API_TOKEN: ${{ secrets.PROWLER_APP_API_TOKEN }}
        run: |
          if [ -z "$PROWLER_APP_API_URL" ] || [ -z "$PROWLER_APP_API_TOKEN" ]; then
            echo "skip publish: PROWLER_APP_API_URL or PROWLER_APP_API_TOKEN is empty"
            exit 0
          fi
          CALLER_ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text 2>/dev/null || echo "")"
          python iac/scripts/publish_scan_to_api.py \
            --input artifacts/rescan/rescan_summary.json \
            --event rescan_verify \
            --api-url "$PROWLER_APP_API_URL" \
            --api-token "$PROWLER_APP_API_TOKEN" \
            --repo "${{ github.repository }}" \
            --run-id "${{ github.run_id }}" \
            --account-id "$CALLER_ACCOUNT_ID" \
            --region "ap-northeast-2" \
            --framework "cis_1.4_plus_isms_p"

      - name: Upload rescan artifacts
        uses: actions/upload-artifact@v4
        with:
          name: rescan-${{ github.run_id }}
          path: artifacts/rescan/
