name: 보안 파이프라인 - 01 인프라 취약점 스캔 → AWS Prowler ( CIS 1.4 / ISMS-P )

on:
  workflow_dispatch:
    inputs:
      deploy_vulnerable:
        description: "Deploy vulnerable infra before scan"
        required: true
        type: boolean
        default: true
      account_id:
        description: "Target AWS account id"
        required: true
        type: string
      compliance_mode:
        description: "Scan mode"
        required: true
        type: choice
        options:
          - cis_1.4_plus_isms_p
          - cis_1.4_only
        default: cis_1.4_plus_isms_p

concurrency:
  group: security-pipeline-01-scan-${{ github.ref }}
  cancel-in-progress: true

permissions:
  id-token: write
  contents: read

jobs:
  scan:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: ap-northeast-2
      AWS_ACCOUNT_ID: ${{ inputs.account_id }}
      TF_VAR_region: ap-northeast-2
      TF_VAR_account_id: ${{ inputs.account_id }}
      TF_VAR_mode: vuln
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
          aws-region: ap-northeast-2

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Deploy vulnerable infra
        if: inputs.deploy_vulnerable == true
        shell: bash
        run: |
          set -euo pipefail
          STATE_BUCKET="prowler-terraform-state-${AWS_ACCOUNT_ID}"
          STATE_KEY="vulnerable_infra_test/terraform.tfstate"

          # 1. State bucket already exists (prowler-terraform-state-{account})
          echo "==> using state bucket: $STATE_BUCKET"

          # 2. Terraform init with S3 backend
          echo "==> terraform init (S3 backend)"
          terraform -chdir=terraform/vulnerable_infra_test init -input=false -reconfigure \
            -backend-config="bucket=${STATE_BUCKET}" \
            -backend-config="key=${STATE_KEY}" \
            -backend-config="region=${AWS_REGION}"

          # 3. Destroy previous run (uses stored state, idempotent)
          # - force_destroy=true on S3 buckets: Terraform empties + deletes them
          # - || true: no-op on first run; tolerates KMS pending deletion
          echo "==> terraform destroy (clean previous state)"
          terraform -chdir=terraform/vulnerable_infra_test destroy \
            -auto-approve -input=false \
            -var account_id="${{ inputs.account_id }}" \
            -var region="${AWS_REGION}" || true

          # 4. Fallback CLI cleanup (orphaned resources outside Terraform state)
          set +e
          echo "==> fallback: cloudtrail trails (orgtrail-*)"
          mapfile -t TRAILS < <(
            aws cloudtrail list-trails --region "$AWS_REGION" \
              --query "Trails[?starts_with(Name,'orgtrail-')].Name" \
              --output text | tr '\t' '\n' | sed '/^$/d'
          )
          for t in "${TRAILS[@]}"; do
            aws cloudtrail delete-trail --region "$AWS_REGION" --name "$t" || true
          done
          sleep 5

          echo "==> fallback: iam users"
          mapfile -t IAM_USERS < <(
            aws iam list-users \
              --query "Users[?starts_with(UserName,'svc-audit-')].UserName" \
              --output text | tr '\t' '\n' | sed '/^$/d'
          )
          for uname in "${IAM_USERS[@]}"; do
            mapfile -t POLS < <(
              aws iam list-attached-user-policies --user-name "$uname" \
                --query "AttachedPolicies[].PolicyArn" --output text | tr '\t' '\n' | sed '/^$/d'
            )
            for arn in "${POLS[@]}"; do
              aws iam detach-user-policy --user-name "$uname" --policy-arn "$arn" 2>/dev/null || true
            done
            aws iam delete-user --user-name "$uname" 2>/dev/null || true
          done
          mapfile -t IAM_POLS < <(
            aws iam list-policies --scope Local \
              --query "Policies[?starts_with(PolicyName,'pol-audit-')].Arn" \
              --output text | tr '\t' '\n' | sed '/^$/d'
          )
          for arn in "${IAM_POLS[@]}"; do
            aws iam delete-policy --policy-arn "$arn" 2>/dev/null || true
          done

          echo "==> fallback: cloudwatch log groups"
          mapfile -t LOG_GROUPS < <(
            aws logs describe-log-groups --region "$AWS_REGION" \
              --query "logGroups[?starts_with(logGroupName, '/ops/audit-')].logGroupName" \
              --output text | tr '\t' '\n' | sed '/^$/d'
          )
          for lg in "${LOG_GROUPS[@]}"; do
            aws logs delete-log-group --region "$AWS_REGION" --log-group-name "$lg" || true
          done

          echo "==> fallback: s3 buckets"
          mapfile -t BUCKETS < <(
            aws s3api list-buckets \
              --query "Buckets[?starts_with(Name,'audit-logs-')||starts_with(Name,'trail-logs-')||starts_with(Name,'alb-logs-')].Name" \
              --output text | tr '\t' '\n' | sed '/^$/d'
          )
          for b in "${BUCKETS[@]}"; do
            echo "  force-delete $b"
            aws s3 rm "s3://$b" --recursive || true
            while IFS=$'\t' read -r key vid; do
              [ -n "$key" ] && aws s3api delete-object \
                --bucket "$b" --key "$key" --version-id "$vid" 2>/dev/null || true
            done < <(aws s3api list-object-versions --bucket "$b" \
              --query 'Versions[].{Key:Key,VersionId:VersionId}' \
              --output text 2>/dev/null)
            while IFS=$'\t' read -r key vid; do
              [ -n "$key" ] && aws s3api delete-object \
                --bucket "$b" --key "$key" --version-id "$vid" 2>/dev/null || true
            done < <(aws s3api list-object-versions --bucket "$b" \
              --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' \
              --output text 2>/dev/null)
            aws s3 rb "s3://$b" --force || true
          done

          echo "==> fallback: vpcs"
          mapfile -t VPCS < <(
            aws ec2 describe-vpcs --region "$AWS_REGION" \
              --filters Name=tag:ProwlerDemo,Values=vulnerable_infra_test \
              --query 'Vpcs[].VpcId' \
              --output text | tr '\t' '\n' | sed '/^$/d'
          )
          for vpc_id in "${VPCS[@]}"; do
            echo "  cleanup vpc $vpc_id"
            mapfile -t SNS < <(aws ec2 describe-subnets --region "$AWS_REGION" \
              --filters Name=vpc-id,Values="$vpc_id" \
              --query 'Subnets[].SubnetId' --output text | tr '\t' '\n' | sed '/^$/d')
            for sn in "${SNS[@]}"; do
              aws ec2 delete-subnet --region "$AWS_REGION" --subnet-id "$sn" || true
            done
            mapfile -t IGWS < <(aws ec2 describe-internet-gateways --region "$AWS_REGION" \
              --filters Name=attachment.vpc-id,Values="$vpc_id" \
              --query 'InternetGateways[].InternetGatewayId' \
              --output text | tr '\t' '\n' | sed '/^$/d')
            for igw in "${IGWS[@]}"; do
              aws ec2 detach-internet-gateway --region "$AWS_REGION" \
                --internet-gateway-id "$igw" --vpc-id "$vpc_id" || true
              aws ec2 delete-internet-gateway --region "$AWS_REGION" \
                --internet-gateway-id "$igw" || true
            done
            mapfile -t RTBS < <(aws ec2 describe-route-tables --region "$AWS_REGION" \
              --filters Name=vpc-id,Values="$vpc_id" Name=association.main,Values=false \
              --query 'RouteTables[].RouteTableId' --output text | tr '\t' '\n' | sed '/^$/d')
            for rtb in "${RTBS[@]}"; do
              aws ec2 delete-route-table --region "$AWS_REGION" --route-table-id "$rtb" || true
            done
            mapfile -t CSGS < <(aws ec2 describe-security-groups --region "$AWS_REGION" \
              --filters Name=vpc-id,Values="$vpc_id" \
              --query "SecurityGroups[?GroupName!='default'].GroupId" \
              --output text | tr '\t' '\n' | sed '/^$/d')
            for sg in "${CSGS[@]}"; do
              aws ec2 delete-security-group --region "$AWS_REGION" --group-id "$sg" || true
            done
            aws ec2 delete-vpc --region "$AWS_REGION" --vpc-id "$vpc_id" || true
          done

          echo "==> fallback: kms keys"
          mapfile -t KMS_ARNS < <(
            aws resourcegroupstaggingapi get-resources --region "$AWS_REGION" \
              --tag-filters Key=ProwlerDemo,Values=vulnerable_infra_test \
              --resource-type-filters kms \
              --query "ResourceTagMappingList[].ResourceARN" \
              --output text 2>/dev/null | tr '\t' '\n' | sed '/^$/d'
          )
          for key_arn in "${KMS_ARNS[@]}"; do
            key_id="${key_arn##*/}"
            aws kms schedule-key-deletion --region "$AWS_REGION" \
              --key-id "$key_id" --pending-window-in-days 7 || true
          done
          set -e

          # 5. Deploy fresh vulnerable infra
          echo "==> terraform apply"
          terraform -chdir=terraform/vulnerable_infra_test apply \
            -auto-approve -input=false \
            -var account_id="${{ inputs.account_id }}" \
            -var region="${AWS_REGION}"


      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install tooling
        run: |
          python -m pip install --upgrade pip
          pip install "prowler>=3.11,<4.0"

      - name: Run Prowler (Seoul + CIS/ISMS-P)
        run: |
          mkdir -p artifacts/scan
          prowler aws --region ap-northeast-2 --compliance cis_1.4_aws -M json-asff -o artifacts/scan -F baseline_cis || true
          if [ "${{ inputs.compliance_mode }}" = "cis_1.4_plus_isms_p" ]; then
            CHECKS="$(tr '\n' ' ' < iac/compliance/isms_p_checks.txt | xargs)"
            if [ -n "$CHECKS" ]; then
              prowler aws --region ap-northeast-2 -c $CHECKS -M json-asff -o artifacts/scan -F baseline_isms_p || true
            fi
          fi
          prowler aws --region ap-northeast-2 --services rds,efs,elb,elbv2,sns,sqs -M json-asff -o artifacts/scan -F baseline_services || true
          python iac/scripts/merge_prowler_outputs.py \
            --dir artifacts/scan \
            --prefix baseline \
            --output artifacts/scan/baseline.asff.json

      - name: Normalize and score findings
        run: |
          python iac/scripts/normalize_findings.py \
            --input artifacts/scan/baseline.asff.json \
            --output artifacts/scan/normalized_findings.json \
            --account-id "${{ inputs.account_id }}" \
            --region "ap-northeast-2"
          python iac/scripts/osfp_score.py \
            --input artifacts/scan/normalized_findings.json \
            --output artifacts/scan/prioritized_findings.json

      - name: Build scan manifest
        run: |
          python iac/scripts/write_scan_manifest.py \
            --normalized artifacts/scan/normalized_findings.json \
            --prioritized artifacts/scan/prioritized_findings.json \
            --output artifacts/scan/scan_manifest.json \
            --account-id "${{ inputs.account_id }}" \
            --region "ap-northeast-2" \
            --framework "${{ inputs.compliance_mode }}"

      - name: Build pipeline summary payload
        run: |
          python iac/scripts/build_pipeline_summary.py \
            --base artifacts/scan/scan_manifest.json \
            --asff artifacts/scan/baseline.asff.json \
            --output artifacts/scan/pipeline_summary.json

      - name: Trigger Prowler native scan (self-hosted)
        continue-on-error: true
        env:
          PROWLER_NATIVE_API_URL: ${{ secrets.PROWLER_NATIVE_API_URL }}
          PROWLER_NATIVE_EMAIL: ${{ secrets.PROWLER_NATIVE_EMAIL }}
          PROWLER_NATIVE_PASSWORD: ${{ secrets.PROWLER_NATIVE_PASSWORD }}
          PROWLER_NATIVE_PROVIDER_NAME: prowler-auto
        run: |
          if [ -z "$PROWLER_NATIVE_API_URL" ] || [ -z "$PROWLER_NATIVE_EMAIL" ] || [ -z "$PROWLER_NATIVE_PASSWORD" ]; then
            echo "skip: PROWLER_NATIVE_API_URL / PROWLER_NATIVE_EMAIL / PROWLER_NATIVE_PASSWORD not set"
            exit 0
          fi
          bash iac/scripts/prowler_native_scan_trigger.sh

      - name: Optional publish baseline scan to external API
        continue-on-error: true
        env:
          PROWLER_APP_API_URL: ${{ secrets.PROWLER_APP_API_URL }}
          PROWLER_APP_API_TOKEN: ${{ secrets.PROWLER_APP_API_TOKEN }}
        run: |
          if [ -z "$PROWLER_APP_API_URL" ]; then
            echo "skip publish: PROWLER_APP_API_URL is empty"
            exit 0
          fi
          python iac/scripts/publish_scan_to_api.py \
            --input artifacts/scan/pipeline_summary.json \
            --event baseline_scan \
            --api-url "$PROWLER_APP_API_URL" \
            --api-token "$PROWLER_APP_API_TOKEN" \
            --repo "${{ github.repository }}" \
            --run-id "${{ github.run_id }}" \
            --account-id "${{ inputs.account_id }}" \
            --region "ap-northeast-2" \
            --framework "${{ inputs.compliance_mode }}"

      - name: Upload scan artifact
        uses: actions/upload-artifact@v4
        with:
          name: scan-${{ github.run_id }}
          path: artifacts/scan/

      - name: Summary
        run: |
          python - <<'PY'
          import glob
          import json
          import re
          from collections import Counter, defaultdict
          from pathlib import Path

          CIS_LOGO_URL = "https://raw.githubusercontent.com/jiwonseok97/prowler-auto-remediation-test/main/.github/assets/cis-logo-2024.png"
          ISMS_LOGO_URL = "https://raw.githubusercontent.com/jiwonseok97/prowler-auto-remediation-test/main/.github/assets/isms-logo.png"

          def parse_asff(pattern):
              findings = []
              for fn in sorted(glob.glob(pattern)):
                  try:
                      data = json.loads(Path(fn).read_text(encoding="utf-8"))
                      if isinstance(data, list):
                          findings.extend(data)
                      elif isinstance(data, dict):
                          findings.extend(data.get("Findings", []))
                  except Exception:
                      pass
              return findings

          def extract_service(finding):
              pf = finding.get("ProductFields", {}) or {}
              service = (pf.get("ServiceName") or pf.get("aws/service") or "").strip().lower()
              if service and service != "unknown":
                  return service
              generator_id = (finding.get("GeneratorId") or "").lower()
              m = re.search(r"prowler-([a-z][a-z0-9]+)_", generator_id)
              return m.group(1) if m else "unknown"

          def title_with_logo(logo_url, alt_text, title, height):
              logo_html = f'<img src="{logo_url}" alt="{alt_text}" height="{height}">'
              return (
                  "<table><tr>"
                  f'<td valign="middle" width="86">{logo_html}</td>'
                  f'<td valign="middle"><strong>{title}</strong></td>'
                  "</tr></table>"
              )

          def make_compliance_table(findings, logo_url, alt_text, title, account_id, logo_height):
              if not findings:
                  return (
                      title_with_logo(logo_url, alt_text, title, logo_height)
                      + "\n\n_No results_\n"
                  )

              svc_data = defaultdict(lambda: {
                  "pass": 0, "fail": 0, "critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0
              })
              local_account_id = account_id
              for finding in findings:
                  if not local_account_id:
                      local_account_id = finding.get("AwsAccountId", "") or ""
                  service = extract_service(finding)
                  status = ((finding.get("Compliance", {}) or {}).get("Status") or "").upper()
                  severity = (((finding.get("Severity", {}) or {}).get("Label") or "").lower())
                  if status == "FAILED":
                      svc_data[service]["fail"] += 1
                      if severity in ("critical", "high", "medium", "low"):
                          svc_data[service][severity] += 1
                      elif severity in ("informational", "info", ""):
                          svc_data[service]["info"] += 1
                      else:
                          # Keep FAIL count and severity columns aligned even for unexpected labels.
                          svc_data[service]["info"] += 1
                  elif status in ("PASSED", "WARNING"):
                      svc_data[service]["pass"] += 1

              total_fail = sum(v["fail"] for v in svc_data.values())
              total_pass = sum(v["pass"] for v in svc_data.values())
              total = total_fail + total_pass
              fail_pct = round((total_fail / total) * 100, 2) if total else 0
              pass_pct = round((total_pass / total) * 100, 2) if total else 0

              lines = [
                  title_with_logo(logo_url, alt_text, title, logo_height),
                  "",
                  "| fail | pass |",
                  "|---|---|",
                  f"| {fail_pct}% ({total_fail}) | {pass_pct}% ({total_pass}) |",
                  "",
              ]
              if local_account_id:
                  lines.append(f"Account `{local_account_id}` summary")
                  lines.append("")

              lines.extend([
                  "| Provider | Service | Status | Critical | High | Medium | Low | Info |",
                  "|---|---|:---:|---:|---:|---:|---:|---:|",
              ])
              for service in sorted(svc_data):
                  v = svc_data[service]
                  is_fail = v["fail"] > 0
                  # Pad counts for visual alignment in the Status column.
                  if is_fail:
                      status_text = f'fail ({v["fail"]:>3})'
                  else:
                      status_text = f'pass ({v["pass"]:>3})'
                  if is_fail:
                      sev_cols = f'{v["critical"]} | {v["high"]} | {v["medium"]} | {v["low"]} | {v["info"]}'
                  else:
                      sev_cols = '- | - | - | - | -'
                  lines.append(
                      f'| aws | `{service}` | {status_text} | {sev_cols} |'
                  )
              return "\n".join(lines)

          p = Path('artifacts/scan/prioritized_findings.json')
          rows = json.loads(p.read_text(encoding='utf-8')) if p.exists() else []
          fail_rows = [r for r in rows if r.get('status') == 'FAIL']
          by_check = Counter(str(r.get('check_id', 'unknown')) for r in fail_rows)
          fail_count = len(fail_rows)

          manifest = Path('artifacts/scan/scan_manifest.json')
          account_id = ""
          if manifest.exists():
              try:
                  manifest_data = json.loads(manifest.read_text(encoding='utf-8'))
                  account_id = manifest_data.get("account") or manifest_data.get("account_id") or ""
              except Exception:
                  account_id = ""

          cis_findings = parse_asff("artifacts/scan/baseline_cis*.json")
          isms_findings = parse_asff("artifacts/scan/baseline_isms_p*.json")
          svc_findings = parse_asff("artifacts/scan/baseline_services*.json")

          check_lines = [
              "| Check ID | FAIL Count |",
              "|---|---:|",
          ]
          for check_id, count in by_check.most_common():
              check_lines.append(f"| `{check_id}` | {count} |")

          with open(Path.cwd() / 'summary.md', 'w', encoding='utf-8') as f:
              f.write("## 01. 취약 인프라 초기 보안 점검 결과\\n")
              f.write(f"- Total FAIL items: {fail_count}\n\n")

              if cis_findings:
                  f.write(make_compliance_table(
                      cis_findings,
                      CIS_LOGO_URL,
                      "CIS",
                      "CIS Amazon Web Services Foundations Benchmark v1.4",
                      account_id,
                      36,
                  ) + "\n\n")

              if isms_findings:
                  f.write(make_compliance_table(
                      isms_findings,
                      ISMS_LOGO_URL,
                      "ISMS-P",
                      "ISMS-P 정보보호 관리체계 인증",
                      account_id,
                      50,
                  ) + "\n\n")

              if svc_findings:
                  f.write(make_compliance_table(
                      svc_findings,
                      CIS_LOGO_URL,
                      "SERVICES",
                      "Additional Services (RDS/EFS/ELB/SNS/SQS)",
                      account_id,
                      30,
                  ) + "\n\n")

              f.write("### 보안 항목별 취약점(FAIL) 현황\\n")
              f.write("\n".join(check_lines) + "\n")
          PY
          cat summary.md >> "$GITHUB_STEP_SUMMARY"


